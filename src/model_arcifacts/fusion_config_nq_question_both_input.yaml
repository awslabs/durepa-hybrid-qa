do_train: true
do_eval: true

# data
data:
  data_dir: null
  max_source_length: 150
  max_target_length: 32
  max_generation_length: 64
  overwrite_cache: false
  output_dir: null
  question_type: nq
  passage_type: tabular
  num_beams: 1
  num_return_sequences: 1
  enable_sql_supervision: false
  cand_for_each_source: 50

# model
model:
  config_name: null
  model_name: t5-base
  tokenizer_name: null
  cache_dir: /tmp/cache/
  use_fast: true
  checkpoint_dir: null
  model_checkpoint: null

# optim
optim:
  train_batch_size: 16
  dev_batch_size: 64
  test_batch_size: 64
  warmup_steps: 0.1
  weight_decay: 0.01
  learning_rate: 1e-4
  adam_epsilon: 1e-8
  seed: 42
  logging_dir: /tmp/wandb/

# trainer
trainer:
  gpus: '0,1,2,3,4,5,6,7'  # use all gpus
  distributed_backend: dp
  val_check_interval: 0.50
  max_steps: 10000
  accumulate_grad_batches: 2
  gradient_clip_val: 1.0
  precision: 16
